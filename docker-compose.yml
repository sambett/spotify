version: '3.8'

services:
  # ============================================================================
  # DATA INGESTION & PROCESSING (Spark + Delta Lake)
  # ============================================================================

  # Scheduled data ingestion service (runs every 6 hours)
  spotify-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spotify-scheduler
    command: python3 scheduler.py
    restart: unless-stopped
    environment:
      - CLIENT_ID=${CLIENT_ID}
      - CLIENT_SECRET=${CLIENT_SECRET}
      - REDIRECT_URI=${REDIRECT_URI:-http://127.0.0.1:8888/callback}
      - BRONZE_PATH=/app/data/bronze
      - SILVER_PATH=/app/data/silver
      - GOLD_PATH=/app/data/gold
      - KAGGLE_CSV=/app/data/kaggle/dataset.csv
      - TOKEN_PATH=/app/data/.spotify_tokens.json
      - SPARK_MASTER=local[*]
      - SPARK_DRIVER_MEMORY=4g
      - SPARK_EXECUTOR_MEMORY=4g
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - ALLOW_SYNTHETIC=${ALLOW_SYNTHETIC:-true}
    volumes:
      - ./data:/app/data
      - ./clients:/app/clients
      - ./config:/app/config
      - ./loaders:/app/loaders
      - ./mappers:/app/mappers
      - ./schemas:/app/schemas
      - ./scripts:/app/scripts
      - ./gold:/app/gold
      - ./utils:/app/utils
      - ./writers:/app/writers
      - ./run_ingestion.py:/app/run_ingestion.py
      - ./scheduler.py:/app/scheduler.py
    ports:
      - "8888:8888"
    networks:
      - spotify-network
    mem_limit: 6g
    memswap_limit: 6g

  # One-time manual ingestion service
  # Use this for: manual data ingestion, building analytics, running ML models
  # Examples:
  #   docker-compose run --rm spotify-pipeline python3 run_ingestion.py
  #   docker-compose run --rm spotify-pipeline python3 gold/predictive/build_predictive_models.py
  #   docker-compose run --rm spotify-pipeline python3 scripts/sync_gold_to_postgres.py
  spotify-pipeline:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spotify-analytics
    environment:
      - CLIENT_ID=${CLIENT_ID}
      - CLIENT_SECRET=${CLIENT_SECRET}
      - REDIRECT_URI=${REDIRECT_URI:-http://127.0.0.1:8888/callback}
      - BRONZE_PATH=/app/data/bronze
      - SILVER_PATH=/app/data/silver
      - GOLD_PATH=/app/data/gold
      - KAGGLE_CSV=/app/data/kaggle/dataset.csv
      - TOKEN_PATH=/app/data/.spotify_tokens.json
      - SPARK_MASTER=local[*]
      - SPARK_DRIVER_MEMORY=4g
      - SPARK_EXECUTOR_MEMORY=4g
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - ALLOW_SYNTHETIC=${ALLOW_SYNTHETIC:-true}
    volumes:
      - ./data:/app/data
      - ./clients:/app/clients
      - ./config:/app/config
      - ./loaders:/app/loaders
      - ./mappers:/app/mappers
      - ./schemas:/app/schemas
      - ./scripts:/app/scripts
      - ./gold:/app/gold
      - ./utils:/app/utils
      - ./writers:/app/writers
      - ./run_ingestion.py:/app/run_ingestion.py
    ports:
      - "8888:8888"
    networks:
      - spotify-network
    mem_limit: 6g
    memswap_limit: 6g
    profiles:
      - manual  # Only runs when explicitly called

  # ============================================================================
  # QUERY LAYER (Trino for SQL-based analytics)
  # ============================================================================
  # Trino (formerly Presto) - Distributed SQL query engine
  # Queries Delta Lake tables directly without copying data
  # RECOMMENDED: Use Trino in Superset instead of PostgreSQL for analytics
  # Connection: trino://admin@trino:8080/delta/default

  trino:
    image: trinodb/trino:435
    container_name: trino
    ports:
      - "8080:8080"
    volumes:
      - ./trino/catalog:/etc/trino/catalog
      - ./data:/data
    networks:
      - spotify-network
    environment:
      - TRINO_ENVIRONMENT=production
    mem_limit: 4g
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ============================================================================
  # VISUALIZATION (Apache Superset)
  # ============================================================================

  # PostgreSQL for Superset metadata
  # PRIMARY USE: Stores Superset's own data (dashboards, users, charts config)
  # SECONDARY USE: Optional - can sync Gold tables here via sync_gold_to_postgres.py
  # NOTE: For analytics, prefer querying Trino (queries Delta Lake directly)
  postgres:
    image: postgres:15
    container_name: superset-postgres
    environment:
      - POSTGRES_DB=superset
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=superset
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - spotify-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis for Superset caching
  # Speeds up dashboard queries by caching results
  redis:
    image: redis:7-alpine
    container_name: superset-redis
    networks:
      - spotify-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Apache Superset
  superset:
    image: apache/superset:3.0.0
    container_name: superset
    environment:
      - SUPERSET_SECRET_KEY=your_secret_key_here_change_in_production
      - SUPERSET_LOAD_EXAMPLES=no
      - DATABASE_DB=superset
      - DATABASE_HOST=postgres
      - DATABASE_PASSWORD=superset
      - DATABASE_USER=superset
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    ports:
      - "8088:8088"
    volumes:
      - ./superset:/app/superset_home
      - ./data:/data
    networks:
      - spotify-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      trino:
        condition: service_healthy
    command: >
      bash -c "
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Admin --lastname User --email admin@superset.com --password admin &&
      superset init &&
      /usr/bin/run-server.sh
      "
    mem_limit: 4g

  # ============================================================================
  # NOTE: ml-service removed - ML models run via spotify-pipeline container
  # Use: docker-compose run --rm spotify-pipeline python3 gold/predictive/build_predictive_models.py
  # ============================================================================

volumes:
  postgres-data:
    driver: local

networks:
  spotify-network:
    driver: bridge
